# å²©æ€§è¯†åˆ«æ¨¡å‹ä¼˜åŒ–æ€è·¯å®Œæ•´æŒ‡å— ğŸš€

## ğŸ“Š ç‰¹å¾ç»Ÿè®¡

### å½“å‰ç‰¹å¾æ•°é‡ï¼š**çº¦80-90ä¸ª** (ä»åŸæ¥çš„51ä¸ªæ‰©å±•)

| ç‰¹å¾ç±»åˆ« | åŸæœ‰ | æ–°å¢ | æ€»è®¡ | è¯´æ˜ |
|---------|------|------|------|------|
| **åŸå§‹ç‰¹å¾** | 3 | 0 | 3 | SP, GR, AC |
| **æ·±åº¦ç‰¹å¾** | 3 | 2 | 5 | æ–°å¢æ·±åº¦åˆ†å±‚ |
| **æ»‘åŠ¨çª—å£ç»Ÿè®¡** | 18 | 0 | 18 | 3ç‰¹å¾Ã—3çª—å£Ã—2ç»Ÿè®¡ |
| **å·®åˆ†ç‰¹å¾** | 9 | 0 | 9 | 3ç‰¹å¾Ã—3çª—å£ |
| **åœ°è´¨ç‰©ç†ç‰¹å¾** | 4 | 11 | 15 | æ–°å¢Larionov/Steiber/æœ‰æ•ˆå­”éš™åº¦ç­‰ |
| **ç‰¹å¾æ¯”å€¼** | 3 | 8 | 11 | æ–°å¢ä¹˜ç§¯ã€å·®å¼‚ã€ç»¼åˆæŒ‡æ ‡ |
| **æ•°å­¦å˜æ¢** | 6 | 0 | 6 | log/sqrtå˜æ¢ |
| **æ’åºç‰¹å¾** | 3 | 0 | 3 | rankç™¾åˆ†ä½ |
| **äº•å†…æ ‡å‡†åŒ–** | 3 | 0 | 3 | well_normalized |
| **æ¢¯åº¦ç‰¹å¾** | 0 | 12 | 12 | ä¸€é˜¶/äºŒé˜¶æ¢¯åº¦ã€å˜åŒ–ç‡ã€æ–¹å‘ |
| **æ³¢åŠ¨æ€§ç‰¹å¾** | 0 | 9 | 9 | CVã€å±€éƒ¨æå€¼ã€å³°å³°å€¼ |
| **æ€»è®¡** | **52** | **42** | **~94** | |

---

## ğŸ¯ 10å¤§è°ƒä¼˜æ€è·¯ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰

### âœ… ä¼˜å…ˆçº§1ï¼šç‰¹å¾å·¥ç¨‹å¢å¼ºï¼ˆå·²å®æ–½ï¼‰

#### æ–°å¢ç‰¹å¾è¯¦è§£ï¼š

**1. åœ°è´¨ä¸“ä¸šç‰¹å¾ï¼ˆ+11ä¸ªï¼‰**
```python
âœ“ Vsh_larionov      # Larionovè€åœ°å±‚æ³¥è´¨å«é‡å…¬å¼ï¼ˆæ¯”çº¿æ€§æ›´å‡†ç¡®ï¼‰
âœ“ Vsh_steiber       # Steiberæ³¥è´¨å«é‡å…¬å¼
âœ“ PHI_wyllie        # Wyllieå£°æ³¢å­”éš™åº¦å…¬å¼
âœ“ PHI_effective     # æœ‰æ•ˆå­”éš™åº¦ï¼ˆå»é™¤æ³¥è´¨å½±å“ï¼‰
âœ“ permeability_proxy # æ¸—é€æ€§æŒ‡æ ‡ï¼ˆKozeny-Carmanè¿‘ä¼¼ï¼‰
âœ“ GR/SP/AC_normalized # æ ‡å‡†åŒ–æµ‹äº•å€¼
âœ“ SP_baseline_deviation # SPåŸºçº¿æ¼‚ç§»
```

**2. æ¢¯åº¦ç‰¹å¾ï¼ˆ+12ä¸ªï¼‰**
```python
âœ“ {feature}_gradient       # ä¸€é˜¶æ¢¯åº¦ï¼ˆæ›²çº¿æ–œç‡ï¼‰
âœ“ {feature}_gradient2      # äºŒé˜¶æ¢¯åº¦ï¼ˆæ›²ç‡ï¼‰
âœ“ {feature}_gradient_pct   # æ¢¯åº¦å˜åŒ–ç‡
âœ“ {feature}_gradient_sign  # æ¢¯åº¦æ–¹å‘ï¼ˆ-1/0/1ï¼‰
```

**3. æ³¢åŠ¨æ€§ç‰¹å¾ï¼ˆ+9ä¸ªï¼‰**
```python
âœ“ {feature}_cv              # å˜å¼‚ç³»æ•°ï¼ˆç›¸å¯¹æ³¢åŠ¨åº¦ï¼‰
âœ“ {feature}_is_local_max    # æ˜¯å¦ä¸ºå±€éƒ¨æœ€å¤§å€¼
âœ“ {feature}_is_local_min    # æ˜¯å¦ä¸ºå±€éƒ¨æœ€å°å€¼
âœ“ {feature}_peak_to_peak    # å³°å³°å€¼ï¼ˆæŒ¯å¹…ï¼‰
```

**4. äº¤äº’ç‰¹å¾å¢å¼ºï¼ˆ+8ä¸ªï¼‰**
```python
âœ“ GR_AC_product/diff        # ä¹˜ç§¯å’Œå·®å¼‚
âœ“ SP_GR_product/diff
âœ“ SP_AC_product/diff
âœ“ sand_shale_index          # ç ‚æ³¥å²©åˆ¤åˆ«æŒ‡æ•°
âœ“ lithology_index           # ç»¼åˆå²©æ€§æŒ‡æ ‡
```

**é¢„æœŸæå‡**: +0.02~0.04

---

### ğŸ”¥ ä¼˜å…ˆçº§2ï¼šæ ·æœ¬å¹³è¡¡ï¼ˆå¯é€‰å¯ç”¨ï¼‰

#### å®æ–½æ–¹æ³•ï¼š
```python
# åœ¨ train_and_predict æ–¹æ³•ä¸­å–æ¶ˆæ³¨é‡Šï¼š
X_train, y_train = self.balance_samples(X_train, y_train)
```

#### æ•ˆæœåˆ†æï¼š
```
å½“å‰æ ·æœ¬åˆ†å¸ƒï¼ˆä¸¥é‡ä¸å¹³è¡¡ï¼‰:
ç ‚å²©(0):    9989  (26.1%)
ç²‰ç ‚å²©(1):  4764  (12.5%)  â† ä¸¥é‡ä¸è¶³
æ³¥å²©(2):   23472  (61.4%)  â† å ä¸»å¯¼

ä½¿ç”¨SMOTEåï¼ˆå¹³è¡¡ï¼‰:
ç ‚å²©(0):   23472  (33.3%)
ç²‰ç ‚å²©(1): 23472  (33.3%)  â† ä¸Šé‡‡æ ·
æ³¥å²©(2):   23472  (33.3%)
```

**ä¼˜ç‚¹**: æå‡å°‘æ•°ç±»(ç²‰ç ‚å²©)è¯†åˆ«ç‡  
**ç¼ºç‚¹**: å¯èƒ½é™ä½å¤šæ•°ç±»(æ³¥å²©)å‡†ç¡®ç‡ï¼Œæ€»ä½“F1å¯èƒ½ä¸å‡åé™  
**å»ºè®®**: å…ˆæµ‹è¯•ä¸å¹³è¡¡ç‰ˆæœ¬ï¼Œå¦‚æœç²‰ç ‚å²©F1è¿‡ä½å†å¯ç”¨  
**é¢„æœŸæå‡**: +0.01~0.03ï¼ˆä¸ç¡®å®šï¼‰

---

### ğŸ”¥ ä¼˜å…ˆçº§3ï¼šæ¨¡å‹é›†æˆç­–ç•¥ä¼˜åŒ–

#### 3.1 æ·»åŠ æ›´å¤šæ ‘æ¨¡å‹
```python
# åœ¨ train_tree_models_advanced æ–¹æ³•ä¸­æ·»åŠ ï¼š

# 1. HistGradientBoosting
from sklearn.ensemble import HistGradientBoostingClassifier
hgb_params = {
    'max_iter': 2000,
    'learning_rate': 0.01,
    'max_depth': 8,
    'l2_regularization': 0.1,
    'early_stopping': True,
    'validation_fraction': 0.1,
    'random_state': 42
}
hgb_model = HistGradientBoostingClassifier(**hgb_params)

# 2. ExtraTreesï¼ˆå¢åŠ é›†æˆå¤šæ ·æ€§ï¼‰
from sklearn.ensemble import ExtraTreesClassifier
et_params = {
    'n_estimators': 500,
    'max_depth': 15,
    'min_samples_split': 10,
    'min_samples_leaf': 5,
    'random_state': 42,
    'n_jobs': -1
}
et_model = ExtraTreesClassifier(**et_params)
```

**é¢„æœŸæå‡**: +0.01~0.02

#### 3.2 Stackingé›†æˆï¼ˆäºŒå±‚æ¨¡å‹ï¼‰
```python
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_predict

# ç¬¬ä¸€å±‚ï¼šåŸºç¡€æ¨¡å‹ç”Ÿæˆå…ƒç‰¹å¾
def create_meta_features(models, X_train, y_train, X_test):
    meta_train = []
    meta_test = []
    
    for name, model_list in models.items():
        # ç”Ÿæˆè®­ç»ƒé›†å…ƒç‰¹å¾ï¼ˆOOFé¢„æµ‹ï¼‰
        oof_pred = np.zeros((len(X_train), 3))
        for model in model_list:
            oof_pred += model.predict_proba(X_train) / len(model_list)
        meta_train.append(oof_pred)
        
        # ç”Ÿæˆæµ‹è¯•é›†å…ƒç‰¹å¾
        test_pred = np.zeros((len(X_test), 3))
        for model in model_list:
            test_pred += model.predict_proba(X_test) / len(model_list)
        meta_test.append(test_pred)
    
    return np.hstack(meta_train), np.hstack(meta_test)

# ç¬¬äºŒå±‚ï¼šå…ƒå­¦ä¹ å™¨
meta_train, meta_test = create_meta_features(tree_models, X_train, y_train, X_test)
meta_model = LogisticRegression(multi_class='multinomial', max_iter=1000)
meta_model.fit(meta_train, y_train)
final_pred = meta_model.predict(meta_test)
```

**é¢„æœŸæå‡**: +0.015~0.025

---

### ğŸ”¥ ä¼˜å…ˆçº§4ï¼šè¶…å‚æ•°ä¼˜åŒ–ï¼ˆOptunaï¼‰

#### å®æ–½æ–¹æ³•ï¼š
```python
import optuna

def objective(trial):
    # LightGBMè¶…å‚æ•°æœç´¢ç©ºé—´
    params = {
        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.05),
        'num_leaves': trial.suggest_int('num_leaves', 20, 50),
        'max_depth': trial.suggest_int('max_depth', 6, 12),
        'min_child_samples': trial.suggest_int('min_child_samples', 10, 50),
        'subsample': trial.suggest_float('subsample', 0.6, 0.9),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.9),
        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 0.5),
        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 0.5),
    }
    
    # äº¤å‰éªŒè¯è¯„ä¼°
    scores = []
    for train_idx, val_idx in gkf.split(X_train, y_train, groups):
        model = lgb.LGBMClassifier(**params, n_estimators=2000)
        model.fit(X_train.iloc[train_idx], y_train.iloc[train_idx],
                 eval_set=[(X_train.iloc[val_idx], y_train.iloc[val_idx])],
                 callbacks=[lgb.early_stopping(150)])
        pred = model.predict(X_train.iloc[val_idx])
        scores.append(f1_score(y_train.iloc[val_idx], pred, average='macro'))
    
    return np.mean(scores)

# è¿è¡Œä¼˜åŒ–ï¼ˆ100æ¬¡è¯•éªŒï¼‰
study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=100)
print("æœ€ä½³å‚æ•°:", study.best_params)
```

**é¢„æœŸæå‡**: +0.01~0.03  
**æ—¶é—´æˆæœ¬**: 3-6å°æ—¶

---

### ğŸ”¥ ä¼˜å…ˆçº§5ï¼šç¥ç»ç½‘ç»œæ¶æ„ä¼˜åŒ–

#### 5.1 æ›´æ·±çš„TabNetæ¨¡å‹
```python
# å®‰è£…: pip install pytorch-tabnet
from pytorch_tabnet.tab_model import TabNetClassifier

tabnet_params = {
    'n_d': 64,              # å†³ç­–å±‚å®½åº¦
    'n_a': 64,              # æ³¨æ„åŠ›å±‚å®½åº¦
    'n_steps': 5,           # å†³ç­–æ­¥æ•°
    'gamma': 1.5,           # ç‰¹å¾é€‰æ‹©çš„ç¼©æ”¾å› å­
    'n_independent': 2,     # ç‹¬ç«‹GLUå±‚æ•°
    'n_shared': 2,          # å…±äº«GLUå±‚æ•°
    'lambda_sparse': 0.0001, # ç¨€ç–æ­£åˆ™åŒ–
    'optimizer_params': {'lr': 0.02},
    'scheduler_params': {'step_size': 50, 'gamma': 0.9},
    'max_epochs': 200,
    'patience': 20,
    'batch_size': 1024,
    'virtual_batch_size': 256,
}

tabnet = TabNetClassifier(**tabnet_params)
tabnet.fit(
    X_train.values, y_train.values,
    eval_set=[(X_val.values, y_val.values)],
    eval_metric=['accuracy'],
)
```

**é¢„æœŸæå‡**: +0.015~0.025

#### 5.2 åŠ å¼ºTransformer
```python
class EnhancedTransformer(nn.Module):
    def __init__(self, input_dim, num_classes=3):
        super().__init__()
        
        # æ›´æ·±çš„åµŒå…¥
        self.embedding = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.LayerNorm(256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, 256),
            nn.LayerNorm(256),
        )
        
        # 6å±‚Transformer
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=256, 
            nhead=8, 
            dim_feedforward=512,
            dropout=0.2,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=6)
        
        # åˆ†ç±»å¤´
        self.classifier = nn.Sequential(
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, num_classes)
        )
```

**é¢„æœŸæå‡**: +0.01~0.02

---

### ğŸ”¥ ä¼˜å…ˆçº§6ï¼šåå¤„ç†ä¼˜åŒ–

#### 6.1 åœ°è´¨è¿ç»­æ€§çº¦æŸ
```python
def geological_smoothing(predictions, test_df, window=5):
    """åŸºäºå²©æ€§åœ¨æ·±åº¦ä¸Šåº”è¯¥è¿ç»­çš„å‡è®¾è¿›è¡Œå¹³æ»‘"""
    from scipy.ndimage import median_filter
    
    smoothed = predictions.copy()
    
    for well in test_df['WELL'].unique():
        well_mask = test_df['WELL'] == well
        well_indices = np.where(well_mask)[0]
        
        # æŒ‰æ·±åº¦æ’åº
        sorted_indices = well_indices[np.argsort(test_df.loc[well_mask, 'DEPTH'].values)]
        well_preds = predictions[sorted_indices]
        
        # ä¸­å€¼æ»¤æ³¢ï¼ˆå²©æ€§çªå˜å°‘ï¼Œåº”è¯¥è¿ç»­ï¼‰
        smoothed_preds = median_filter(well_preds, size=window)
        smoothed[sorted_indices] = smoothed_preds
    
    return smoothed

# ä½¿ç”¨
final_predictions = geological_smoothing(raw_predictions, test_df, window=5)
```

**é¢„æœŸæå‡**: +0.005~0.015

#### 6.2 è§„åˆ™æ ¡æ­£
```python
def rule_based_correction(predictions, test_df, features):
    """åŸºäºåœ°è´¨è§„åˆ™çš„é¢„æµ‹æ ¡æ­£"""
    corrected = predictions.copy()
    
    # è§„åˆ™1: æé«˜GRå€¼(>150) â†’ æ³¥å²©(2)
    high_gr_mask = test_df['GR'] > 150
    corrected[high_gr_mask] = 2
    
    # è§„åˆ™2: æä½GRå€¼(<50) + é«˜å­”éš™åº¦ â†’ ç ‚å²©(0)
    sandstone_mask = (test_df['GR'] < 50) & (features['PHI_AC'] > 0.15)
    corrected[sandstone_mask] = 0
    
    # è§„åˆ™3: ä¸­ç­‰GR(50-100) + ä¸­ç­‰å­”éš™åº¦ â†’ ç²‰ç ‚å²©(1)
    siltstone_mask = (test_df['GR'].between(50, 100)) & (features['PHI_AC'].between(0.08, 0.15))
    corrected[siltstone_mask] = 1
    
    return corrected
```

**é¢„æœŸæå‡**: +0.005~0.01

---

### ğŸ”¥ ä¼˜å…ˆçº§7ï¼šç‰¹å¾é€‰æ‹©

#### 7.1 åŸºäºé‡è¦æ€§çš„ç‰¹å¾ç­›é€‰
```python
from sklearn.feature_selection import SelectKBest, mutual_info_classif

# äº’ä¿¡æ¯ç‰¹å¾é€‰æ‹©
mi_scores = mutual_info_classif(X_train, y_train, random_state=42)
feature_scores = pd.DataFrame({
    'feature': X_train.columns,
    'mi_score': mi_scores
}).sort_values('mi_score', ascending=False)

print("Top 20ç‰¹å¾:")
print(feature_scores.head(20))

# é€‰æ‹©å‰60ä¸ªæœ€é‡è¦çš„ç‰¹å¾
selector = SelectKBest(mutual_info_classif, k=60)
X_train_selected = selector.fit_transform(X_train, y_train)
X_test_selected = selector.transform(X_test)
```

**é¢„æœŸæå‡**: +0.005~0.015ï¼ˆå‡å°‘è¿‡æ‹Ÿåˆï¼‰

#### 7.2 é€’å½’ç‰¹å¾æ¶ˆé™¤ï¼ˆRFEï¼‰
```python
from sklearn.feature_selection import RFECV

# ä½¿ç”¨LightGBMä½œä¸ºåŸºç¡€ä¼°è®¡å™¨
estimator = lgb.LGBMClassifier(n_estimators=100, random_state=42)
rfe = RFECV(estimator, step=5, cv=5, scoring='f1_macro', n_jobs=-1)
rfe.fit(X_train, y_train)

print(f"æœ€ä¼˜ç‰¹å¾æ•°: {rfe.n_features_}")
print(f"é€‰æ‹©çš„ç‰¹å¾: {X_train.columns[rfe.support_].tolist()}")

X_train_rfe = rfe.transform(X_train)
X_test_rfe = rfe.transform(X_test)
```

**é¢„æœŸæå‡**: +0.005~0.02

---

### ğŸ”¥ ä¼˜å…ˆçº§8ï¼šæ•°æ®å¢å¼º

#### 8.1 Mixupï¼ˆé’ˆå¯¹è¡¨æ ¼æ•°æ®ï¼‰
```python
def mixup_data(x, y, alpha=0.2):
    """Mixupæ•°æ®å¢å¼º"""
    if alpha > 0:
        lam = np.random.beta(alpha, alpha)
    else:
        lam = 1
    
    batch_size = len(x)
    index = np.random.permutation(batch_size)
    
    mixed_x = lam * x + (1 - lam) * x[index]
    y_a, y_b = y, y[index]
    
    return mixed_x, y_a, y_b, lam

# åœ¨ç¥ç»ç½‘ç»œè®­ç»ƒä¸­ä½¿ç”¨
for epoch in range(epochs):
    for batch_x, batch_y in train_loader:
        mixed_x, y_a, y_b, lam = mixup_data(batch_x, batch_y)
        outputs = model(mixed_x)
        loss = lam * criterion(outputs, y_a) + (1 - lam) * criterion(outputs, y_b)
```

**é¢„æœŸæå‡**: +0.005~0.01

---

### ğŸ”¥ ä¼˜å…ˆçº§9ï¼šé›†æˆæƒé‡ä¼˜åŒ–

#### ä½¿ç”¨éªŒè¯é›†ä¼˜åŒ–æƒé‡
```python
from scipy.optimize import minimize

def find_optimal_weights(all_predictions, y_val):
    """å¯»æ‰¾æœ€ä¼˜é›†æˆæƒé‡"""
    
    def objective(weights):
        # åŠ æƒé›†æˆ
        ensemble = np.zeros_like(all_predictions[0])
        for pred, w in zip(all_predictions, weights):
            ensemble += pred * w
        
        # è®¡ç®—F1åˆ†æ•°ï¼ˆè´Ÿæ•°ç”¨äºæœ€å°åŒ–ï¼‰
        final_pred = np.argmax(ensemble, axis=1)
        return -f1_score(y_val, final_pred, average='macro')
    
    # çº¦æŸï¼šæƒé‡å’Œä¸º1
    constraints = {'type': 'eq', 'fun': lambda w: np.sum(w) - 1}
    bounds = [(0, 1) for _ in range(len(all_predictions))]
    
    # åˆå§‹æƒé‡ï¼ˆå‡åŒ€ï¼‰
    x0 = np.ones(len(all_predictions)) / len(all_predictions)
    
    # ä¼˜åŒ–
    result = minimize(objective, x0, bounds=bounds, constraints=constraints, method='SLSQP')
    
    return result.x

# ä½¿ç”¨
optimal_weights = find_optimal_weights(val_predictions, y_val)
print(f"ä¼˜åŒ–åæƒé‡: {optimal_weights}")
```

**é¢„æœŸæå‡**: +0.005~0.015

---

### ğŸ”¥ ä¼˜å…ˆçº§10ï¼šä¼ªæ ‡ç­¾ï¼ˆPseudo Labelingï¼‰

#### åŠç›‘ç£å­¦ä¹ ç­–ç•¥
```python
def pseudo_labeling_iteration(models, X_train, y_train, X_test, confidence_threshold=0.95):
    """è¿­ä»£ä¼ªæ ‡ç­¾"""
    
    # 1. é¢„æµ‹æµ‹è¯•é›†
    test_probs = ensemble_predict_proba(models, X_test)
    max_probs = np.max(test_probs, axis=1)
    
    # 2. é€‰æ‹©é«˜ç½®ä¿¡åº¦æ ·æœ¬
    confident_mask = max_probs > confidence_threshold
    pseudo_labels = np.argmax(test_probs[confident_mask], axis=1)
    X_pseudo = X_test[confident_mask]
    
    print(f"æ·»åŠ  {len(X_pseudo)} ä¸ªä¼ªæ ‡ç­¾æ ·æœ¬ (ç½®ä¿¡åº¦>{confidence_threshold})")
    print(f"ä¼ªæ ‡ç­¾åˆ†å¸ƒ: {pd.Series(pseudo_labels).value_counts().to_dict()}")
    
    # 3. åˆå¹¶è®­ç»ƒé›†ï¼ˆç»™ä¼ªæ ‡ç­¾æ›´å°çš„æƒé‡ï¼‰
    sample_weights = np.concatenate([
        np.ones(len(X_train)),           # çœŸå®æ ‡ç­¾æƒé‡=1
        np.ones(len(X_pseudo)) * 0.5      # ä¼ªæ ‡ç­¾æƒé‡=0.5
    ])
    
    X_combined = np.vstack([X_train, X_pseudo])
    y_combined = np.hstack([y_train, pseudo_labels])
    
    # 4. é‡æ–°è®­ç»ƒ
    return X_combined, y_combined, sample_weights

# ä½¿ç”¨ï¼ˆå¯ä»¥è¿­ä»£2-3æ¬¡ï¼‰
for iteration in range(2):
    print(f"\nä¼ªæ ‡ç­¾è¿­ä»£ {iteration+1}")
    X_train, y_train, weights = pseudo_labeling_iteration(
        models, X_train, y_train, X_test, confidence_threshold=0.95
    )
    models = retrain_models(X_train, y_train, sample_weight=weights)
```

**é¢„æœŸæå‡**: +0.01~0.03  
**é£é™©**: å¯èƒ½æ”¾å¤§é”™è¯¯

---

## ğŸ“ˆ ç»¼åˆæå‡é¢„æœŸ

| ä¼˜åŒ–ç­–ç•¥ | é¢„æœŸæå‡ | å®æ–½éš¾åº¦ | æ—¶é—´æˆæœ¬ |
|---------|---------|---------|---------|
| âœ… ç‰¹å¾å·¥ç¨‹å¢å¼º | **+0.02~0.04** | â­â­â­ | å·²å®Œæˆ |
| æ ·æœ¬å¹³è¡¡(SMOTE) | +0.01~0.03 | â­â­ | 5åˆ†é’Ÿ |
| æ·»åŠ HistGB/ExtraTrees | +0.01~0.02 | â­â­ | 15åˆ†é’Ÿ |
| Stackingé›†æˆ | +0.015~0.025 | â­â­â­ | 30åˆ†é’Ÿ |
| è¶…å‚æ•°ä¼˜åŒ–(Optuna) | +0.01~0.03 | â­â­â­ | 3-6å°æ—¶ |
| TabNetæ¨¡å‹ | +0.015~0.025 | â­â­â­â­ | 1å°æ—¶ |
| åœ°è´¨å¹³æ»‘åå¤„ç† | +0.005~0.015 | â­â­ | 10åˆ†é’Ÿ |
| ç‰¹å¾é€‰æ‹© | +0.005~0.015 | â­â­ | 20åˆ†é’Ÿ |
| é›†æˆæƒé‡ä¼˜åŒ– | +0.005~0.015 | â­â­ | 10åˆ†é’Ÿ |
| ä¼ªæ ‡ç­¾ | +0.01~0.03 | â­â­â­â­ | 30åˆ†é’Ÿ |

---

## ğŸ¯ å¿«é€Ÿæåˆ†è·¯çº¿ï¼ˆæŒ‰æ—¶é—´åˆ†é…ï¼‰

### 30åˆ†é’Ÿå†…ï¼ˆ+0.025~0.04ï¼‰
1. âœ… ç‰¹å¾å·¥ç¨‹å¢å¼ºï¼ˆå·²å®Œæˆï¼‰
2. åœ°è´¨å¹³æ»‘åå¤„ç†ï¼ˆ10åˆ†é’Ÿï¼‰
3. é›†æˆæƒé‡ä¼˜åŒ–ï¼ˆ10åˆ†é’Ÿï¼‰
4. æ ·æœ¬å¹³è¡¡æµ‹è¯•ï¼ˆ10åˆ†é’Ÿï¼‰

### 1å°æ—¶å†…ï¼ˆ+0.04~0.07ï¼‰
5. æ·»åŠ HistGB/ExtraTreesï¼ˆ15åˆ†é’Ÿï¼‰
6. Stackingé›†æˆï¼ˆ30åˆ†é’Ÿï¼‰
7. ç‰¹å¾é€‰æ‹©ï¼ˆ15åˆ†é’Ÿï¼‰

### 3å°æ—¶å†…ï¼ˆ+0.05~0.10ï¼‰
8. TabNetæ¨¡å‹ï¼ˆ1å°æ—¶ï¼‰
9. ä¼ªæ ‡ç­¾è¿­ä»£ï¼ˆ30åˆ†é’Ÿï¼‰
10. è¶…å‚æ•°ç²¾è°ƒï¼ˆ1.5å°æ—¶ï¼‰

---

## ğŸ” è°ƒè¯•æŠ€å·§

### 1. æ£€æŸ¥ç‰¹å¾é‡è¦æ€§
```python
# LightGBMç‰¹å¾é‡è¦æ€§
lgb_importance = pd.DataFrame({
    'feature': feature_names,
    'importance': model.feature_importances_
}).sort_values('importance', ascending=False)

print(lgb_importance.head(30))
```

### 2. åˆ†æé”™è¯¯æ¡ˆä¾‹
```python
# æ‰¾å‡ºé¢„æµ‹é”™è¯¯æœ€å¤šçš„æ ·æœ¬
val_pred = model.predict(X_val)
errors = y_val != val_pred

error_analysis = pd.DataFrame({
    'true_label': y_val[errors],
    'pred_label': val_pred[errors],
    'GR': X_val.loc[errors, 'GR'],
    'SP': X_val.loc[errors, 'SP'],
    'AC': X_val.loc[errors, 'AC'],
})

print("é”™è¯¯æ ·æœ¬åˆ†æ:")
print(error_analysis.groupby(['true_label', 'pred_label']).size())
```

### 3. æ··æ·†çŸ©é˜µåˆ†æ
```python
from sklearn.metrics import confusion_matrix
import seaborn as sns

cm = confusion_matrix(y_val, val_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
```

---

## ğŸ“ æ³¨æ„äº‹é¡¹

1. **é˜²æ­¢è¿‡æ‹Ÿåˆ**: ç‰¹å¾è¶Šå¤šï¼Œè¿‡æ‹Ÿåˆé£é™©è¶Šå¤§ï¼Œéœ€è¦ï¼š
   - å¢åŠ æ­£åˆ™åŒ–
   - ç‰¹å¾é€‰æ‹©
   - æ›´å¤šæ•°æ®å¢å¼º

2. **è®¡ç®—èµ„æº**: 94ä¸ªç‰¹å¾ä¼šæ˜¾è‘—å¢åŠ è®­ç»ƒæ—¶é—´
   - Mac MPS: é¢„è®¡1-2å°æ—¶
   - CUDA GPU: é¢„è®¡30-60åˆ†é’Ÿ

3. **æ ·æœ¬å¹³è¡¡**: ä¸ä¸€å®šæœ‰æ•ˆï¼Œå»ºè®®å…ˆæµ‹è¯•
   - å¦‚æœç²‰ç ‚å²©F1<0.3ï¼Œè€ƒè™‘å¯ç”¨
   - å¦‚æœæ€»ä½“F1ä¸‹é™ï¼Œæ”¾å¼ƒ

4. **ä¼ªæ ‡ç­¾é£é™©**: ç½®ä¿¡åº¦é˜ˆå€¼è¦é«˜(>0.95)
   - å¤ªä½ä¼šå¼•å…¥å™ªå£°
   - å¤ªé«˜é€‰ä¸åˆ°è¶³å¤Ÿæ ·æœ¬

---

## ğŸ† é¢„æœŸæœ€ç»ˆåˆ†æ•°

| é˜¶æ®µ | åˆ†æ•° | æå‡ |
|------|------|------|
| å½“å‰Baseline | 0.628 | - |
| ç‰¹å¾å·¥ç¨‹å¢å¼º | 0.648~0.668 | +0.02~0.04 |
| æ·»åŠ æ¨¡å‹+Stacking | 0.663~0.693 | +0.015~0.025 |
| åå¤„ç†+ä¼ªæ ‡ç­¾ | 0.678~0.723 | +0.015~0.03 |
| **æœ€ç»ˆé¢„æœŸ** | **0.70~0.75** | **+0.072~0.122** |

---

## ğŸ“¦ éœ€è¦å®‰è£…çš„åŒ…

```bash
# æ ·æœ¬å¹³è¡¡
pip install imbalanced-learn

# è¶…å‚æ•°ä¼˜åŒ–
pip install optuna

# TabNet
pip install pytorch-tabnet

# å¯è§†åŒ–
pip install seaborn matplotlib

# å…¶ä»–å·¥å…·
pip install scipy scikit-learn
```

---

**Good Luck! ğŸš€**
